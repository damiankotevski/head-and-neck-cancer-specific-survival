{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION USING LOGISTIC REGRESSION, RANDOM FOREST, GRADIENT BOOSTED TREES, SUPPORT VECTOR MACHINE, ARTIFICIAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/PATH/TO/FILE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "# import BayesianRidge model\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imputer\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), # default\n",
    "                           n_nearest_features=None, # default\n",
    "                           imputation_order='ascending', # uses features with fewest missing values first\n",
    "                           initial_strategy='most_frequent') # used for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the imputer on the dataset\n",
    "imputer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataset\n",
    "transformed_df = imputer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(transformed_df).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create complete dataframe with imputed data\n",
    "complete_df = pd.DataFrame(imputer.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.columns = [#insert column names#]\n",
    "print(complete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_df = complete_df.round()\n",
    "print(completed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = IterativeImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1])]\n",
    "X, y = data[:, ix], data[:, 14] # based on 'Variable' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)'% (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test imputation on one variable\n",
    "grade_unrounded = complete_df['Variable'].value_counts() #insert veriable name\n",
    "print(\"Frequency of Variable\")\n",
    "print(grade_unrounded)\n",
    "grade_rounded = completed_df['Variable'].value_counts()\n",
    "print(\"Frequency of Variable\")\n",
    "print(grade_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVER SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into input and output features\n",
    "X = completed_df.drop(axis=1, columns=['Variable']) #drop outcome variable and other variables not to be used in prediction\n",
    "y = completed_df['Variable'] #enter outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create balanced dataset\n",
    "smote = SMOTE(random_state=1, k_neighbors=5)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset is balanced\n",
    "print(f'''Shape of X before SMOTE: {X.shape}\n",
    "Shape of X after SMOTE: {X_smote.shape}''')\n",
    "print(f'''Shape of y before SMOTE: {y.shape}\n",
    "Shape of y after SMOTE: {y_smote.shape}''')\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "y_smote.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set, default 20% test\n",
    "# set random_state = 0 for reproducibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=0, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries for gridsearch and metrics etc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression model\n",
    "Log_Reg = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gridsearchCV to find the best parameters for the linear model\n",
    "# define values for parameter tuning, change as needed \n",
    "penalty = ['l1', 'l2']\n",
    "C = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "class_weight = [{0:.1, 1:.9}, {0:.2, 1:.8}, {0:.3, 1:.7}, {0:.4, 1:.6}, {0:.5, 1:.5}]\n",
    "max_iter = [50, 100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Log Reg parameter dictionary for tuning\n",
    "Log_Reg_param_grid = dict(penalty=penalty,\n",
    "                          C=C, \n",
    "                          class_weight=class_weight,\n",
    "                          max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Log Reg gridsearchCV object with 5 CV\n",
    "grid_search_Log_Reg = GridSearchCV(Log_Reg, param_grid=Log_Reg_param_grid, n_jobs=-1, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the Log Reg gridsearchCV object\n",
    "grid_search_Log_Reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best Log Reg parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_Log_Reg.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_Log_Reg.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using tuned log reg model on train set\n",
    "y_pred_Log_Reg_grid_search_train= grid_search_Log_Reg.predict(X_train)\n",
    "\n",
    "# predict outcome using tuned log reg model on train set\n",
    "y_pred_Log_Reg_grid_search = grid_search_Log_Reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_Log_Reg_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_Log_Reg_grid_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance metrics of the tuned linear model\n",
    "# construct the confusion matrix\n",
    "cm_Log_Reg = confusion_matrix(y_true = y_test, y_pred = y_pred_Log_Reg_grid_search)\n",
    "\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the LogReg:\")\n",
    "print(cm_Log_Reg)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the LogReg:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_Log_Reg)\n",
    "plt.title('Confusion matrix of the LogReg:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using logistic regression (l2) model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_Log_Reg_grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot area under curve for LogReg model\n",
    "y_pred_proba_Log_Reg = grid_search_Log_Reg.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for Log Reg:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_Log_Reg))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_Log_Reg)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve LogReg')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREE BASED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a gradient boosted tree model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbt = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gridsearchCV to find the best parameters for the gradient boosted tree model\n",
    "# define values for parameter tuning, change as needed  \n",
    "learning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.5, 1]\n",
    "n_estimators = [100, 500, 1000, 2000, 5000]\n",
    "max_depth = [2, 4, 6, 8, 10]\n",
    "min_samples_split = [2, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gradient boosted tree parameter dictionary for tuning\n",
    "gbt_param_grid = dict(learning_rate=learning_rate,\n",
    "                            n_estimators=n_estimators, \n",
    "                            max_depth=max_depth,\n",
    "                            min_samples_split=min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the gradient boosted tree gridsearchCV object with 5 CV\n",
    "grid_search_gbt = GridSearchCV(gbt, param_grid=gbt_param_grid, cv=5, n_jobs=-1, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tree gridsearchCV object\n",
    "grid_search_gbt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best gradient boosted tree parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_gbt.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_gbt.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using tuned gradient boosted model on train set\n",
    "y_pred_gbt_grid_search_train= grid_search_gbt.predict(X_train)\n",
    "\n",
    "# predict outcome using tuned gradient boost model on train set\n",
    "y_pred_gbt_grid_search = grid_search_gbt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_gbt_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_gbt_grid_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance metrics of the tuned gbt model\n",
    "# construct the confusion matrix\n",
    "cm_gbt = confusion_matrix(y_true = y_test, y_pred = y_pred_gbt_grid_search)\n",
    "\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the gbt:\")\n",
    "print(cm_gbt)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the gbt:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_gbt)\n",
    "plt.title('Confusion matrix of the gbt:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using gradient boosted tree model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_gbt_grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot area under curve for gbt model\n",
    "y_pred_proba_gbt = grid_search_gbt.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for gradient boosted tree:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_gbt))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_gbt)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve gradient boosted tree')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a random forrest tree model\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "RF = RandomForestClassifier(random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gridsearchCV to find the best parameters for the random forest tree model\n",
    "# define values for parameter tuning, change as needed  \n",
    "max_features = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "n_estimators = [1000, 2000, 5000, 10000]\n",
    "max_depth = [2, 4, 6, 8, 10]\n",
    "class_weight = [{0:.1, 1:.9}, {0:.2, 1:.8}, {0:.3, 1:.7}, {0:.4, 1:.6}, {0:.5, 1:.5}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest tree parameter dictionary for tuning\n",
    "RF_param_grid = dict(max_features=max_features,\n",
    "                            n_estimators=n_estimators, \n",
    "                            max_depth=max_depth,\n",
    "                            class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the random forest tree gridsearchCV object with 5 CV\n",
    "grid_search_RF = GridSearchCV(RF, param_grid=RF_param_grid, cv=5, n_jobs=-1, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tree gridsearchCV object\n",
    "grid_search_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best random forest tree parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_RF.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_RF.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using tuned random forest model on train set\n",
    "y_pred_RF_grid_search_train= grid_search_RF.predict(X_train)\n",
    "\n",
    "# predict outcome using tuned RF model on train set\n",
    "y_pred_RF_grid_search = grid_search_RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_RF_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_RF_grid_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance metrics of the tuned random forest model\n",
    "# construct the confusion matrix\n",
    "cm_RF = confusion_matrix(y_true = y_test, y_pred = y_pred_RF_grid_search)\n",
    "\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the RF:\")\n",
    "print(cm_RF)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the RF:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_RF)\n",
    "plt.title('Confusion matrix of the RF:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using random forest tree model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_RF_grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot area under curve for RF model\n",
    "y_pred_proba_RF = grid_search_RF.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for random forest tree:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_RF))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_RF)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve random forest tree')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a SVM model with scaled data \n",
    "from sklearn.svm import *\n",
    "svm = SVC(probability=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gridsearchCV to find the best parameters for the svm model\n",
    "# define values for parameter tuning, change as needed  \n",
    "C = [1/100000, 1/10000, 1/1000, 1/100, 1/10]\n",
    "kernel = ['linear', 'sigmoid', 'poly', 'rbf'] \n",
    "gamma = [1, 1/2, 1/3, 1/5, 1/10, 1/50, 1/100]\n",
    "class_weight = [{0:.1, 1:.9}, {0:.2, 1:.8}, {0:.3, 1:.7}, {0:.4, 1:.6}, {0:.5, 1:.5}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create svm parameter dictionary for tuning\n",
    "svm_param_grid = dict(C=C, \n",
    "                      kernel=kernel, \n",
    "                      gamma=gamma,\n",
    "                      class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the svm gridsearchCV object with 5 CV with accuracy\n",
    "grid_search_svm = GridSearchCV(svm, param_grid=svm_param_grid, cv=5, n_jobs=-1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the svm gridsearchCV object\n",
    "grid_search_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best svm parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_svm.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_svm.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using tuned svm model on train set\n",
    "y_pred_svm_grid_search_train= grid_search_svm.predict(X_train)\n",
    "\n",
    "# predict outcome using tuned svm model on train set\n",
    "y_pred_svm_grid_search = grid_search_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_svm_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_svm_grid_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance metrics of the tuned svm model\n",
    "# construct the confusion matrix\n",
    "cm_svm = confusion_matrix(y_true = y_test, y_pred = y_pred_svm_grid_search)\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the svm:\")\n",
    "print(cm_svm)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the svm:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_svm)\n",
    "plt.title('Confusion matrix of the svm:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using svm model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_svm_grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot area under curve for svm model\n",
    "y_pred_proba_svm = grid_search_svm.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for svm:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_svm))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_svm)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve svm')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTIFICIAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed to enable experiment to be reproducible, randomly selected 2\n",
    "from numpy.random import seed\n",
    "seed = 2\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile ANN model with randomly selected parameters, parameters will be tuned later\n",
    "def create_model(dropout_rate=0.1, \n",
    "                 weight_constraint=1, \n",
    "                 optimizer='sgd',\n",
    "                 learn_rate=0.01,\n",
    "                 momentum=0.1,\n",
    "                 neurons=10,\n",
    "                 activation='relu'):\n",
    "    model = Sequential()\n",
    "    # input_dim = number of imput variables\n",
    "    model.add(Dense(neurons, input_dim=23, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap Keras model so it can be used by scikit-learn \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "ann = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define values for each hyperparameter for tuning, change as needed \n",
    "dropout_rate = [0.005, 0.01, 0.05]\n",
    "epochs = [5, 10, 15, 20]\n",
    "batch_size = [50, 100, 500, 1000]\n",
    "optimizer = ['SGD'] # select SGD as default for time cost reduction\n",
    "learn_rate = [0.001, 0.01, 0.1]\n",
    "momentum = [0.0001, 0.001, 0.01, 0.1]\n",
    "neurons = [5, 10, 15, 20]\n",
    "activation = ['sigmoid'] # select sigmoid for binary output\n",
    "\n",
    "ann_param_grid = dict(dropout_rate=dropout_rate, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs,\n",
    "                  optimizer=optimizer, \n",
    "                  learn_rate=learn_rate, \n",
    "                  momentum=momentum, \n",
    "                  neurons=neurons, \n",
    "                  activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the svm gridsearchCV object with 5 CV with accuracy score\n",
    "grid_search_ann = GridSearchCV(ann, param_grid=ann_param_grid, cv=5, n_jobs=-1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit grid search to training set\n",
    "grid_search_ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best ann parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_ann.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_ann.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using tuned ann model on train set\n",
    "y_pred_ann_grid_search_train= grid_search_ann.predict(X_train)\n",
    "\n",
    "# predict outcome using tuned ann model on train set\n",
    "y_pred_ann_grid_search = grid_search_ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_ann_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_ann_grid_search)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance metrics of the tuned ann model\n",
    "# construct the confusion matrix\n",
    "cm_ann = confusion_matrix(y_true = y_test, y_pred = y_pred_ann_grid_search)\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the ann:\")\n",
    "print(cm_ann)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the ann:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_ann)\n",
    "plt.title('Confusion matrix of the ann:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using ann model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_ann_grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot area under curve for ann model\n",
    "y_pred_proba_ann = grid_search_ann.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for ann:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_ann))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_ann)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve ann')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISE PREDICTORS USING DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(decision_tree.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(decision_tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(decision_tree, out_file=\"tree.dot\", class_names=[\"Outcome Present\", \"Outcome Absent\"], feature_names=X.columns, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to also install graphviz on system https://www.graphviz.org/download/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'#'C:/Users/............../graphviz-2.38/release/bin/'\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(decision_tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance = pd.DataFrame(decision_tree.feature_importances_, index=X_smote.columns, columns=['feature importance']).sort_values('feature importance', ascending=True)\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_feature_importance.plot.barh()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
