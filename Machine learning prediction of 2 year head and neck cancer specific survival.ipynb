{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION USING LOGISTIC REGRESSION, RANDOM FOREST, GRADIENT BOOSTED TREES, SUPPORT VECTOR MACHINE, ARTIFICIAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library requirements\n",
    "# pip install sklearn\n",
    "# pip install imbalanced-learn\n",
    "# pip install delayed\n",
    "# pip install graphviz\n",
    "# pip install keras\n",
    "# pip install tensorflow --user\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import isnan\n",
    "from sklearn import metrics\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import *\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from IPython.display import display\n",
    "from plotnine import *\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz, DecisionTreeClassifier\n",
    "import graphviz # need to install graphviz on system https://www.graphviz.org/download/ \n",
    "import os\n",
    "\n",
    "# open dataframe\n",
    "df = pd.read_csv(\"/PATH/TO/FILE.csv\")\n",
    "# define imputer\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), # default\n",
    "                           n_nearest_features=None, # default\n",
    "                           imputation_order='ascending', # uses features with fewest missing values first\n",
    "                           initial_strategy='most_frequent') # used for categorical data\n",
    "# fit the imputer on the dataset\n",
    "imputer.fit(df)\n",
    "# transform the dataset\n",
    "transformed_df = imputer.transform(df)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(transformed_df).flatten()))\n",
    "# create complete dataframe with imputed data\n",
    "complete_df = pd.DataFrame(imputer.transform(df))\n",
    "complete_df.columns = [#insert column names#]\n",
    "completed_df = complete_df.round()\n",
    "# define the modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = IterativeImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "# split into input and output elements\n",
    "data = df.values\n",
    "ix = [i for i in range(data.shape[1])]\n",
    "X, y = data[:, ix], data[:, 1] # 1 is based on 'Variable' column\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)'% (scores.mean(), scores.std()))\n",
    "# test imputation on one variable\n",
    "grade_unrounded = complete_df['Variable'].value_counts() #insert veriable name\n",
    "# print(\"Frequency of Variable\")\n",
    "# print(grade_unrounded)\n",
    "grade_rounded = completed_df['Variable'].value_counts()\n",
    "# print(\"Frequency of Variable\")\n",
    "# print(grade_rounded)\n",
    "\n",
    "# split the dataset into input and output features\n",
    "X = completed_df.drop(axis=1, columns=['Variable']) #drop outcome variable and other variables not to be used in prediction\n",
    "y = completed_df['Variable'] #enter outcome variable\n",
    "# create balanced dataset\n",
    "smote = SMOTE(random_state=1, k_neighbors=5)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "# check dataset is balanced\n",
    "# print(f'''Shape of X before SMOTE: {X.shape}\n",
    "# Shape of X after SMOTE: {X_smote.shape}''')\n",
    "# print(f'''Shape of y before SMOTE: {y.shape}\n",
    "# Shape of y after SMOTE: {y_smote.shape}''')\n",
    "# print('\\nBalance of positive and negative classes (%):')\n",
    "y_smote.value_counts(normalize=True) * 100\n",
    "\n",
    "# split data into train and test set, default 20% test, set random_state = 0 for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=0, test_size = 0.2)\n",
    "\n",
    "# create logistic regression model\n",
    "Log_Reg = LogisticRegression(random_state=0)\n",
    "# use gridsearchCV to find the best parameters for the linear model, define values for parameter tuning, change as needed \n",
    "penalty = ['l1', 'l2']\n",
    "C = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "class_weight = [{0:.1, 1:.9}, {0:.2, 1:.8}, {0:.3, 1:.7}, {0:.4, 1:.6}, {0:.5, 1:.5}]\n",
    "max_iter = [50, 100, 200, 500]\n",
    "# create Log Reg parameter dictionary for tuning\n",
    "Log_Reg_param_grid = dict(penalty=penalty,\n",
    "                          C=C, \n",
    "                          class_weight=class_weight,\n",
    "                          max_iter=max_iter)\n",
    "# define the Log Reg gridsearchCV object with 5 CV\n",
    "grid_search_Log_Reg = GridSearchCV(Log_Reg, param_grid=Log_Reg_param_grid, n_jobs=-1, cv=5, scoring='accuracy')\n",
    "# fit the Log Reg gridsearchCV object\n",
    "grid_search_Log_Reg.fit(X_train, y_train)\n",
    "# print best Log Reg parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_Log_Reg.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_Log_Reg.best_score_))\n",
    "# predict outcome using tuned log reg model on train set\n",
    "y_pred_Log_Reg_grid_search_train= grid_search_Log_Reg.predict(X_train)\n",
    "# predict outcome using tuned log reg model on train set\n",
    "y_pred_Log_Reg_grid_search = grid_search_Log_Reg.predict(X_test)\n",
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_Log_Reg_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_Log_Reg_grid_search)))\n",
    "# evaluate performance metrics of the tuned linear model\n",
    "# construct the confusion matrix\n",
    "cm_Log_Reg = confusion_matrix(y_true = y_test, y_pred = y_pred_Log_Reg_grid_search)\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the LogReg:\")\n",
    "print(cm_Log_Reg)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the LogReg:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_Log_Reg)\n",
    "plt.title('Confusion matrix of the LogReg:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using logistic regression (l2) model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_Log_Reg_grid_search))\n",
    "# plot area under curve for LogReg model\n",
    "y_pred_proba_Log_Reg = grid_search_Log_Reg.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for Log Reg:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_Log_Reg))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_Log_Reg)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve LogReg')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "# fit a gradient boosted tree model\n",
    "gbt = GradientBoostingClassifier(random_state=0)\n",
    "# use gridsearchCV to find the best parameters for the gradient boosted tree model, define values for parameter tuning, change as needed  \n",
    "learning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.5, 1]\n",
    "n_estimators = [100, 500, 1000, 2000, 5000]\n",
    "max_depth = [2, 4, 6, 8, 10]\n",
    "min_samples_split = [2, 5, 10]\n",
    "# create gradient boosted tree parameter dictionary for tuning\n",
    "gbt_param_grid = dict(learning_rate=learning_rate,\n",
    "                            n_estimators=n_estimators, \n",
    "                            max_depth=max_depth,\n",
    "                            min_samples_split=min_samples_split)\n",
    "# define the gradient boosted tree gridsearchCV object with 5 CV\n",
    "grid_search_gbt = GridSearchCV(gbt, param_grid=gbt_param_grid, cv=5, n_jobs=-1, scoring = 'accuracy')\n",
    "# fit the tree gridsearchCV object\n",
    "grid_search_gbt.fit(X_train, y_train)\n",
    "# print best gradient boosted tree parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_gbt.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_gbt.best_score_))\n",
    "# predict outcome using tuned gradient boosted model on train set\n",
    "y_pred_gbt_grid_search_train= grid_search_gbt.predict(X_train)\n",
    "# predict outcome using tuned gradient boost model on train set\n",
    "y_pred_gbt_grid_search = grid_search_gbt.predict(X_test)\n",
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_gbt_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_gbt_grid_search)))\n",
    "# evaluate performance metrics of the tuned gbt model\n",
    "# construct the confusion matrix\n",
    "cm_gbt = confusion_matrix(y_true = y_test, y_pred = y_pred_gbt_grid_search)\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the gbt:\")\n",
    "print(cm_gbt)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the gbt:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_gbt)\n",
    "plt.title('Confusion matrix of the gbt:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using gradient boosted tree model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_gbt_grid_search))\n",
    "# plot area under curve for gbt model\n",
    "y_pred_proba_gbt = grid_search_gbt.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for gradient boosted tree:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_gbt))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_gbt)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve gradient boosted tree')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "# fit a random forrest tree model\n",
    "RF = RandomForestClassifier(random_state=0) \n",
    "# use gridsearchCV to find the best parameters for the random forest tree model, define values for parameter tuning, change as needed  \n",
    "max_features = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "n_estimators = [1000, 2000, 5000, 10000]\n",
    "max_depth = [2, 4, 6, 8, 10]\n",
    "class_weight = [{0:.1, 1:.9}, {0:.2, 1:.8}, {0:.3, 1:.7}, {0:.4, 1:.6}, {0:.5, 1:.5}]\n",
    "# create random forest tree parameter dictionary for tuning\n",
    "RF_param_grid = dict(max_features=max_features,\n",
    "                            n_estimators=n_estimators, \n",
    "                            max_depth=max_depth,\n",
    "                            class_weight=class_weight)\n",
    "# define the random forest tree gridsearchCV object with 5 CV\n",
    "grid_search_RF = GridSearchCV(RF, param_grid=RF_param_grid, cv=5, n_jobs=-1, scoring = 'accuracy')\n",
    "# fit the tree gridsearchCV object\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "# print best random forest tree parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_RF.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_RF.best_score_))\n",
    "# predict outcome using tuned random forest model on train set\n",
    "y_pred_RF_grid_search_train= grid_search_RF.predict(X_train)\n",
    "# predict outcome using tuned RF model on train set\n",
    "y_pred_RF_grid_search = grid_search_RF.predict(X_test)\n",
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_RF_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_RF_grid_search)))\n",
    "# evaluate performance metrics of the tuned random forest model\n",
    "# construct the confusion matrix\n",
    "cm_RF = confusion_matrix(y_true = y_test, y_pred = y_pred_RF_grid_search)\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the RF:\")\n",
    "print(cm_RF)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the RF:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_RF)\n",
    "plt.title('Confusion matrix of the RF:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using random forest tree model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_RF_grid_search))\n",
    "# plot area under curve for RF model\n",
    "y_pred_proba_RF = grid_search_RF.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for random forest tree:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_RF))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_RF)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve random forest tree')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "# fit a SVM model with scaled data \n",
    "svm = SVC(probability=True, random_state=0)\n",
    "# use gridsearchCV to find the best parameters for the svm model, define values for parameter tuning, change as needed  \n",
    "C = [1/100000, 1/10000, 1/1000, 1/100, 1/10]\n",
    "kernel = ['linear', 'sigmoid', 'poly', 'rbf'] \n",
    "gamma = [1, 1/2, 1/3, 1/5, 1/10, 1/50, 1/100]\n",
    "class_weight = [{0:.1, 1:.9}, {0:.2, 1:.8}, {0:.3, 1:.7}, {0:.4, 1:.6}, {0:.5, 1:.5}]\n",
    "# create random forest tree parameter dictionary for tuning\n",
    "svm_param_grid = dict(C=C,\n",
    "                        kernel=kernel, \n",
    "                        gamma=gamma,\n",
    "                        class_weight=class_weight)\n",
    "# define the svm gridsearchCV object with 5 CV with accuracy\n",
    "grid_search_svm = GridSearchCV(svm, param_grid=svm_param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "# fit the svm gridsearchCV object\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "# print best svm parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_svm.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_svm.best_score_))\n",
    "# predict outcome using tuned svm model on train set\n",
    "y_pred_svm_grid_search_train= grid_search_svm.predict(X_train)\n",
    "# predict outcome using tuned svm model on train set\n",
    "y_pred_svm_grid_search = grid_search_svm.predict(X_test)\n",
    "# print the accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_svm_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_svm_grid_search)))\n",
    "# evaluate performance metrics of the tuned svm model\n",
    "# construct the confusion matrix\n",
    "cm_svm = confusion_matrix(y_true = y_test, y_pred = y_pred_svm_grid_search)\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the svm:\")\n",
    "print(cm_svm)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the svm:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_svm)\n",
    "plt.title('Confusion matrix of the svm:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "# Print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using svm model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_svm_grid_search))\n",
    "# plot area under curve for svm model\n",
    "y_pred_proba_svm = grid_search_svm.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for svm:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_svm))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_svm)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve svm')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "# fix random seed to enable experiment to be reproducible, randomly selected 2\n",
    "seed = 2\n",
    "tf.random.set_seed(2)\n",
    "# create and compile ANN model with randomly selected parameters, parameters will be tuned later\n",
    "def create_model(dropout_rate=0.1, \n",
    "                 weight_constraint=1, \n",
    "                 optimizer='sgd',\n",
    "                 learn_rate=0.01,\n",
    "                 momentum=0.1,\n",
    "                 neurons=10,\n",
    "                 activation='relu'):\n",
    "    model = Sequential()\n",
    "    # input_dim = number of imput variables\n",
    "    model.add(Dense(neurons, input_dim=23, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# wrap Keras model so it can be used by scikit-learn \n",
    "ann = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define values for each hyperparameter for tuning, change as needed \n",
    "dropout_rate = [0.005, 0.01, 0.05]\n",
    "epochs = [5, 10, 15, 20]\n",
    "batch_size = [50, 100, 500, 1000]\n",
    "optimizer = ['SGD'] # select SGD as default for time cost reduction\n",
    "learn_rate = [0.001, 0.01, 0.1]\n",
    "momentum = [0.0001, 0.001, 0.01, 0.1]\n",
    "neurons = [5, 10, 15, 20]\n",
    "activation = ['sigmoid'] # select sigmoid for binary output\n",
    "\n",
    "ann_param_grid = dict(dropout_rate=dropout_rate, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs,\n",
    "                  optimizer=optimizer, \n",
    "                  learn_rate=learn_rate, \n",
    "                  momentum=momentum, \n",
    "                  neurons=neurons, \n",
    "                  activation=activation)\n",
    "# define the svm gridsearchCV object with 5 CV with accuracy score\n",
    "grid_search_ann = GridSearchCV(ann, param_grid=ann_param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "# fit grid search to training set\n",
    "grid_search_ann.fit(X_train, y_train)\n",
    "# print best ann parameters and average accuracy score\n",
    "print(\"Best parameters: {}\".format(grid_search_ann.best_params_))\n",
    "print(\"Best cross-validation average accuracy score: {:.2f}\".format(grid_search_ann.best_score_))\n",
    "# predict outcome using tuned ann model on train set\n",
    "y_pred_ann_grid_search_train= grid_search_ann.predict(X_train)\n",
    "# predict outcome using tuned ann model on train set\n",
    "y_pred_ann_grid_search = grid_search_ann.predict(X_test)\n",
    "# print accuracy scores\n",
    "print(\"Accuracy on training set: {:.2f}\".format(accuracy_score(y_train, y_pred_ann_grid_search_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred_ann_grid_search)))\n",
    "# evaluate performance metrics of the tuned ann model\n",
    "# construct the confusion matrix\n",
    "cm_ann = confusion_matrix(y_true = y_test, y_pred = y_pred_ann_grid_search)\n",
    "# plot confusion matrix\n",
    "print(\"Confusion matrix plot of the ann:\")\n",
    "print(cm_ann)\n",
    "print()\n",
    "print(\"Confusion matrix plot of the ann:\")\n",
    "disease_labels = ['Outcome Absent', 'Outcome Present']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm_ann)\n",
    "plt.title('Confusion matrix of the ann:')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + disease_labels)\n",
    "ax.set_yticklabels([''] + disease_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "# print classification report\n",
    "print(\"Precision, Recall, F1-score for class 0 (Outcome Absent) and class 1 (Outcome Present) classes of the test set:\")\n",
    "print(\"Using ann model\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_ann_grid_search))\n",
    "# plot area under curve for ann model\n",
    "y_pred_proba_ann = grid_search_ann.predict_proba(X_test)[:,1]\n",
    "print(\"Area under curve for ann:\")\n",
    "print(metrics.roc_auc_score(y_true = y_test, y_score = y_pred_proba_ann))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba_ann)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve ann')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "# visualse decision tree\n",
    "decision_tree = grid_search_RF\n",
    "print(\"Accuracy on training set: {:.3f}\".format(decision_tree.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(decision_tree.score(X_test, y_test)))\n",
    "export_graphviz(decision_tree, out_file=\"tree.dot\", class_names=[\"Outcome Present\", \"Outcome Absent\"], feature_names=X.columns, impurity=False, filled=True)\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'#'C:/Users/............../graphviz-2.38/release/bin/'\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n",
    "# plot feature importances\n",
    "print(\"Feature importances:\\n{}\".format(decision_tree.feature_importances_))\n",
    "df_feature_importance = pd.DataFrame(decision_tree.feature_importances_, index=X_smote.columns, columns=['feature importance']).sort_values('feature importance', ascending=True)\n",
    "df_feature_importance\n",
    "ax = df_feature_importance.plot.barh()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
